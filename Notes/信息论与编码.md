# 信息论与编码

## Chapter 2 entropy，relative entropy，mutual information

### 2.1 entropy（熵）

### 2.2 joint entropy（联合熵）, conditional entropy（条件熵）

### 2.3 relative entropy（相对熵）, mutual information（互信息量）

### 2.4 熵与互信息量的关系

$$
I(X;Y)\newline
=H(X)-H(X|Y) \newline
=H(Y)-H(Y|X) \newline
=H(X)+H(Y)-H(X,Y) \newline
=I(Y;X)
$$

$$
I(X;X)=H(X)
$$

### 2.5 熵、相对熵、互信息量的链式法则

(1) **熵的链式法则**：引入条件熵

一组随机变量的熵（联合熵）等于条件熵之和
$$
H(X_1,X_2,...,X_n)=\sum_{i=1}^{n}{H(X_i|X_{i-1},...,X_1)}
$$
随机变量X和Y在给定随机变量Z时的条件互信息量（conditional mutual information），它是在给定Z时由于Y的知识而引起关于X的不确定度的缩减量：
$$
I(X;Y|Z)=H(X|Z)-H(X|Y,Z)\newline
=E_{p(x,y,z)}{log\frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}}\newline
\newline
I(X_1,X_2;Y)=E_{p(x_1,x_2,y)}{log\frac{p(X_1,X_2,Y)}{p(X_1,X_2)p(Y)}}\newline
\newline
I(X_1,X_2,..,X_n;Y_1,Y_2,...,Y_m|Z_1,Z_2,...,Z_k)\newline
=E_{p(x_1,..,x_n,y_1,...,y_m,Z_1,...,Z_k)}{log\frac{p(X_1,..,X_n,Y_1,...,Y_m|Z_1,...,Z_k)}{p(X_1,..,X_n|Z_1,...,Z_k)p(Y_1,...,Y_m|Z_1,...,Z_k)}}
$$
(2) **互信息量的链式法则**：引入条件互信息量
$$
I(X_1,X_2,...,X_n;Y)=\sum_{i=1}^{n}{I(X_i;Y|X_{i-1},...,X_1)}
$$
(3) **相对熵的链式法则**：引入条件相对熵

### 2.6 Jensen不等式及其结果
